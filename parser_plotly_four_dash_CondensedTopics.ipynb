{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "ERROR: Could not find a version that satisfies the requirement matplotlib_inline.pyplot (from versions: none)\n",
      "ERROR: No matching distribution found for matplotlib_inline.pyplot\n",
      "WARNING: You are using pip version 21.1.2; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bensg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\bensg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with package installation\n",
      "loading outlook messages\n",
      "applying outlook filter\n",
      "building core network arrays\n",
      "error processing message =  <COMObject <unknown>>\n",
      "(682, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bensg\\AppData\\Local\\Temp\\ipykernel_24164\\4158547288.py:118: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n",
      "C:\\Users\\bensg\\AppData\\Local\\Temp\\ipykernel_24164\\4158547288.py:268: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "\n",
      "C:\\Users\\bensg\\AppData\\Local\\Temp\\ipykernel_24164\\4158547288.py:268: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "\n",
      "c:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning:\n",
      "\n",
      "The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record size :  4173\n",
      "merging topic colors\n",
      "building network node array\n",
      "building network edge array\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001B[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001B[0m\n",
      "\u001B[2m   Use a production WSGI server instead.\u001B[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bensg\\AppData\\Local\\Temp\\ipykernel_24164\\4158547288.py:268: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "\n",
      "C:\\Users\\bensg\\AppData\\Local\\Temp\\ipykernel_24164\\4158547288.py:268: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "\n",
      "C:\\Users\\bensg\\AppData\\Local\\Temp\\ipykernel_24164\\4158547288.py:245: FutureWarning:\n",
      "\n",
      "The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "\n",
      "C:\\Users\\bensg\\AppData\\Local\\Temp\\ipykernel_24164\\4158547288.py:268: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "\n",
      "C:\\Users\\bensg\\AppData\\Local\\Temp\\ipykernel_24164\\4158547288.py:268: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "\n",
      "C:\\Users\\bensg\\AppData\\Local\\Temp\\ipykernel_24164\\4158547288.py:268: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Mar/2022 11:08:44] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Mar/2022 11:08:44] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Mar/2022 11:08:44] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Mar/2022 11:08:44] \"GET /_dash-component-suites/dash/dcc/async-dropdown.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Mar/2022 11:08:44] \"GET /_dash-component-suites/dash/dash_table/async-table.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Mar/2022 11:08:44] \"GET /_dash-component-suites/dash/dash_table/async-highlight.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Mar/2022 11:08:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Mar/2022 11:08:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Mar/2022 11:08:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Mar/2022 11:08:46] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Mar/2022 11:08:47] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "\n",
    "def import_or_install(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        pip.main(['install', package])\n",
    "\n",
    "packages = ['PyQt6', 'nltk', 'pyvis' , 'matplotlib_inline' ,'matplotlib', 'gensim.corpora','gensim.utils','gensim.models' , 'matplotlib_inline.pyplot' ,'pandas', 'numpy', 'umap','sqlite3','spacy','win32com.client','datetime','pyvis.network','matplotlib.pyplot','plotly.graph_objects','scipy','networkx','dash','gensim','logging','warnings','nltk.corpus']\n",
    "\n",
    "\n",
    "for package in packages:\n",
    "    import_or_install(package)\n",
    "\n",
    "print('done with package installation')\n",
    "\n",
    "import nltk\n",
    "import sqlite3\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "import win32com.client\n",
    "from datetime import datetime, timedelta\n",
    "from dash import html\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import dash\n",
    "import json\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash import dash_table\n",
    "import dash_cytoscape as cyto\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "def load_outlook_messages(folder_number):\n",
    "    print('loading outlook messages')\n",
    "    outlook = win32com.client.Dispatch(\"Outlook.Application\").GetNamespace(\"MAPI\")\n",
    "    folder_contents = outlook.GetDefaultFolder(folder_number)\n",
    "    outlook_messages = folder_contents.Items\n",
    "    return outlook_messages\n",
    "\n",
    "def outlook_message_filter(message_array, date_range):\n",
    "    print('applying outlook filter')\n",
    "    #filter messages processed by the last X days\n",
    "    received_dt = datetime.now() - timedelta(days=date_range)\n",
    "    received_dt = received_dt.strftime('%m/%d/%Y %H:%M %p')\n",
    "    message_array = message_array.Restrict(\"[ReceivedTime] >= '\" + received_dt + \"'\")\n",
    "    return message_array\n",
    "\n",
    "def build_core_network_arrays(messages):\n",
    "    print('building core network arrays')\n",
    "    #Build core tables\n",
    "    pd_messages = []\n",
    "    pd_recipients = []\n",
    "    for message in list(messages):\n",
    "        try:\n",
    "            recipients_list = message.Recipients\n",
    "            recipients_cleaned = \"\"\n",
    "            for recipient in recipients_list:\n",
    "                pd_recipients.append({\n",
    "                    \"entryID\": str(message.EntryID),\n",
    "                    \"sender\": str(message.Sender),\n",
    "                    \"recipient\": str(recipient),\n",
    "                })\n",
    "\n",
    "            pd_messages.append({\n",
    "                \"entryID\": str(message.EntryID),\n",
    "                \"conversationID\": str(message.ConversationID),\n",
    "                \"conversationIndex\": str(message.ConversationIndex),\n",
    "                \"createTime\": str(message.CreationTime),\n",
    "                \"recievedTime\": str(message.ReceivedTime),\n",
    "                \"ConversationTopic\": str(message.ConversationTopic),\n",
    "                \"subject\": str(message.Subject),\n",
    "                \"body\": str(message.body)\n",
    "            })\n",
    "        except:\n",
    "            print(\"error processing message =  \" + str(recipients_list))\n",
    "\n",
    "    return pd_messages, pd_recipients\n",
    "\n",
    "def core_arrays_to_pd(pd_messages, pd_recipients):\n",
    "    #Can probably get rid of this if I move it into the network array builder\n",
    "    df_messages = pd.DataFrame(pd_messages)\n",
    "    df_recipients = pd.DataFrame(pd_recipients)\n",
    "    return df_messages, df_recipients\n",
    "\n",
    "def outlook_messages_to_pd(folder, days_back):\n",
    "    outlook_messages = load_outlook_messages(folder)\n",
    "    messages = outlook_message_filter(outlook_messages, days_back)\n",
    "    pd_messages, pd_recipients = build_core_network_arrays(messages)\n",
    "    df_messages, df_recipients = core_arrays_to_pd(pd_messages, pd_recipients)\n",
    "    print(df_messages.shape)\n",
    "    return df_messages, df_recipients\n",
    "\n",
    "def lemmatize_with_postag(sentence):\n",
    "    #Get each words Part of speech ( verb, noun etc) then pass it with the word to a lemmatizer. Then put the lemmatized word back into the Pandas DF row\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a',\n",
    "                \"N\": 'n',\n",
    "                \"V\": 'v',\n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]\n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "def set_lowercase_and_drop_punctuation(pd_dataframe):\n",
    "    #dataframe needs to have a body col which it will work on\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['body'].apply(lambda x: x.lower())\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].str.replace(r'[^\\w\\s]+', '')  #(?:\\w+)\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].replace(r'\\n', ' ', regex=True)\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].replace(r'\\r', ' ', regex=True)\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].replace(r'_', '', regex=True)\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].str.strip()\n",
    "    return pd_dataframe\n",
    "\n",
    "def lem_and_stop_removal(pd_dataframe):\n",
    "    #Remove stopwords & lemmatize\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].apply(lemmatize_with_postag)\n",
    "    return pd_dataframe\n",
    "\n",
    "def normalize_df_col_text(pd_dataframe):\n",
    "    #Text normalization bundled function\n",
    "    df_messages = set_lowercase_and_drop_punctuation(pd_dataframe)\n",
    "    pd_dataframe = lem_and_stop_removal(df_messages)\n",
    "    return pd_dataframe\n",
    "\n",
    "def create_topics(pd_messages, topic_count, tfidf, random_state):\n",
    "    dtm = tfidf.fit_transform(df_messages['temp_body'])\n",
    "    nmf_model = NMF(n_components=topic_count, random_state=random_state)\n",
    "    nmf_model.fit(dtm)\n",
    "    print(\"record size : \", len(tfidf.get_feature_names_out()))\n",
    "    return nmf_model, dtm\n",
    "\n",
    "def topics_to_pd(dtm, nmf_model, tfidf, topic_count):\n",
    "    topics_dict={}\n",
    "    for index, topic in enumerate(nmf_model.components_):\n",
    "        topic_text_string =  str([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-topic_count:]])\n",
    "        v = {index: str(topic_text_string)}\n",
    "        topics_dict.update(v)\n",
    "\n",
    "    topic_text_df =  pd.Series(topics_dict).to_frame()\n",
    "    topic_text_df.reset_index(level=0, inplace=True)\n",
    "    topic_text_df = topic_text_df.rename(columns={0: \"topic_text\", 'index': \"topic\"})\n",
    "    topic_results = nmf_model.transform(dtm)\n",
    "    df_messages['topic'] = topic_results.argmax(axis=1)\n",
    "    return df_messages, topic_text_df\n",
    "\n",
    "def generate_topic_colors(topics_pd):\n",
    "    topic_colors = {}\n",
    "    for x in range(topic_count):\n",
    "        hex_color = \"%06x\" % random.randint(0, 0xFFFFFF)\n",
    "        hex_color = \"#\" + hex_color\n",
    "        topic_colors.update({ x: x , x : hex_color,})\n",
    "\n",
    "    topic_color_df = pd.DataFrame(topic_colors.items(), columns=['topic','color'])\n",
    "    topic_color_df = topic_color_df.merge(topic_text_df, left_on=['topic','topic'], right_on=['topic','topic'],  how='left')\n",
    "    return topic_color_df\n",
    "\n",
    "def build_network_nodes(df_nodes):\n",
    "    print('building network node array')\n",
    "    nodes = set()\n",
    "    cy_nodes = []\n",
    "    for index, row in df_nodes.iterrows():\n",
    "        individual, bins = row['individual'], row['log_count']\n",
    "        nodes.add(individual)\n",
    "        cy_nodes.append({\"data\": {\"id\": individual, \"label\": individual, 'weight': bins*10, }})\n",
    "    return cy_nodes\n",
    "\n",
    "def build_network_edges(df_edges):\n",
    "    print('building network edge array')\n",
    "    cy_edges = []\n",
    "    for index, row in df_edges.iterrows():\n",
    "        source, target, topic, weight, topic_color = row['source'], row['target'], row['topic'], row['size'], row['color']\n",
    "        cy_edges.append({\n",
    "            'data': {\n",
    "                'source': source,\n",
    "                'target': target,\n",
    "                'topic': topic,\n",
    "                'weight': weight,\n",
    "                'topic_color': topic_color\n",
    "            }\n",
    "        })\n",
    "    return cy_edges\n",
    "\n",
    "\n",
    "def generate_table(dataframe, max_rows=200):\n",
    "    return html.Table([\n",
    "        html.Thead(\n",
    "            html.Tr([html.Th(col) for col in dataframe.columns])\n",
    "        ),\n",
    "        html.Tbody([\n",
    "            html.Tr([\n",
    "                 html.Td(dataframe.iloc[i][col]) for col in dataframe.columns\n",
    "            ]) for i in range(min(len(dataframe), max_rows))\n",
    "\n",
    "        ])\n",
    "    ])\n",
    "\n",
    "\n",
    "def group_recipients(recipients):\n",
    "    #must include color\n",
    "    recipients = recipients.groupby(['sender',  'recipient', 'topic', 'color']).size()\n",
    "    recipients = recipients.to_frame(name = 'size').reset_index()\n",
    "    recipients = recipients.loc[recipients['size'] != 1]\n",
    "    return recipients\n",
    "\n",
    "\n",
    "def drop_na_recipients(recipients):\n",
    "    nan_value = float(\"NaN\")\n",
    "    recipients.replace(\"\", nan_value, inplace=True)\n",
    "    recipients = recipients.dropna()\n",
    "    return recipients\n",
    "\n",
    "\n",
    "\n",
    "def merge_topic_colors(messages, recipients, topic_colors):\n",
    "    #THIS CHUNK GENERATES TOPIC COLORS AND MERGES IT WITH THE MASTER TABLEs\n",
    "    print('merging topic colors')\n",
    "    messages = messages.merge(topic_colors, left_on=['topic','topic'], right_on=['topic','topic'],  how='left')\n",
    "    recipients = messages.merge(recipients, left_on=['entryID','entryID'], right_on=['entryID','entryID'],  how='left')\n",
    "    recipients = recipients.reset_index(drop=True)\n",
    "    return messages, recipients\n",
    "\n",
    "\n",
    "def pipeline_topic_colors(messages, recipients, topic_colors):\n",
    "    messages, recipients = merge_topic_colors(messages, recipients, topic_colors)\n",
    "    recipients = group_recipients(recipients)\n",
    "    recipients = drop_na_recipients(recipients)\n",
    "    return messages, recipients\n",
    "\n",
    "\n",
    "def generate_node_dataframe(recipients):\n",
    "    df_node_temp =  recipients\n",
    "    #build node list off of to/from table. Adding a new feature called count driven by node edge counts\n",
    "    df1 = df_node_temp['sender']\n",
    "    df2 = df_node_temp['recipient']\n",
    "    df_nodes = df1.append(df2)\n",
    "    df_nodes.reset_index()\n",
    "    df_nodes = df_nodes.to_frame()\n",
    "    df_nodes = df_nodes.rename(columns={0: \"individuals\"})\n",
    "    df_nodes = df_nodes['individuals'].value_counts()\n",
    "    df_nodes = df_nodes.to_frame()\n",
    "    df_nodes = df_nodes.rename(columns={'individuals': \"count\"})\n",
    "    df_nodes.index.name = 'individual'\n",
    "    df_nodes.reset_index(inplace=True)\n",
    "    return df_nodes\n",
    "\n",
    "\n",
    "def generate_edges_dataframe(recipients):\n",
    "    edges_table = [recipients[\"sender\"], recipients[\"recipient\"], recipients['topic'], recipients['color'], recipients['size']]\n",
    "    headers = [\"source\", \"target\", \"topic\",\"color\", \"size\"]\n",
    "    df_edges = pd.concat(edges_table, axis=1, keys=headers)\n",
    "    return df_edges\n",
    "\n",
    "\n",
    "def load_table_from_sqlite(filename, table_name):\n",
    "    sql_con = sqlite3.connect(filename)\n",
    "    query = 'select * from ' + str(table_name)\n",
    "    df_loaded = pd.read_sql_query(query, sql_con)\n",
    "    df_loaded.drop('index', 1)\n",
    "    sql_con.close()\n",
    "    return df_loaded\n",
    "\n",
    "\n",
    "def save_to_sqlite(sqlite_file, table_to_save, tabel_name):\n",
    "    sql_con = sqlite3.connect(sqlite_file)\n",
    "    table_to_save.to_sql(tabel_name, sql_con, if_exists=\"replace\")\n",
    "    sql_con = sqlite3.connect(sqlite_file)\n",
    "    sql_con.close()\n",
    "\n",
    "\n",
    "#stopword definitions for later sections\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "#Set Driving Vars\n",
    "topic_count = 15   #number of ML generated topics\n",
    "folder_to_load =  6 #which folders to load from outlook\n",
    "days_back_to_look = 100 #number of days back to pull data for\n",
    "sqlite_file_name = 'test_db_sqlite.db'\n",
    "\n",
    "#Grab all messages from outlook folder X that came in during the lasy Y days\n",
    "df_messages, df_recipients = outlook_messages_to_pd(folder_to_load, days_back_to_look)\n",
    "df_messages = normalize_df_col_text(df_messages)\n",
    "\n",
    "#save messages to sqlite\n",
    "save_to_sqlite(sqlite_file_name, df_messages, \"messages\" )\n",
    "save_to_sqlite(sqlite_file_name, df_recipients, \"recipients\" )\n",
    "\n",
    "\n",
    "#load messsages from sqlite\n",
    "df_messages = load_table_from_sqlite(sqlite_file_name,\"messages\")\n",
    "df_recipients = load_table_from_sqlite(sqlite_file_name,\"recipients\")\n",
    "\n",
    "\n",
    "#Generate topics\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', stop_words='english')\n",
    "nmf_model, dtm = create_topics(df_messages, topic_count, tfidf, 42)\n",
    "df_messages, topic_text_df = topics_to_pd(dtm,nmf_model, tfidf, topic_count)\n",
    "topic_color_df = generate_topic_colors(topic_text_df)\n",
    "\n",
    "\n",
    "\n",
    "#Merge topics & topic colors with core network tables & save to db\n",
    "df_messages, df_recipients = pipeline_topic_colors(df_messages, df_recipients, topic_color_df)\n",
    "save_to_sqlite(sqlite_file_name,df_recipients,\"compressed_data\")\n",
    "save_to_sqlite(sqlite_file_name,topic_color_df,\"topic_color_df\")\n",
    "\n",
    "\n",
    "\n",
    "#load up compressed data/save & load point, then generate nodes & edges dataframes\n",
    "df_recipients = load_table_from_sqlite(sqlite_file_name,\"compressed_data\")\n",
    "topic_color_df = load_table_from_sqlite(sqlite_file_name,\"topic_color_df\")\n",
    "\n",
    "\n",
    "df_nodes = generate_node_dataframe(df_recipients)\n",
    "df_edges = generate_edges_dataframe(df_recipients)\n",
    "\n",
    "\n",
    "\n",
    "ser = Series(df_nodes['count'])\n",
    "bins = np.quantile(np.unique(ser), np.linspace(0, 1, 11))\n",
    "\n",
    "bin_count = np.shape(bins)[0]\n",
    "size_labels = range(5, (bin_count + 5) - 1, 1)\n",
    "\n",
    "\n",
    "df_nodes['bins'] = pd.cut(ser, bins, include_lowest=True, labels=size_labels)\n",
    "df_nodes['log_count'] = np.log(df_nodes['count'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "save_to_sqlite(sqlite_file_name, df_nodes, \"df_nodes\")\n",
    "save_to_sqlite(sqlite_file_name, df_edges, \"df_edges\")\n",
    "\n",
    "\n",
    "df_edges = load_table_from_sqlite(sqlite_file_name, \"df_edges\")\n",
    "df_nodes = load_table_from_sqlite(sqlite_file_name, \"df_nodes\")\n",
    "topic_color_df = load_table_from_sqlite(sqlite_file_name, \"topic_color_df\")\n",
    "\n",
    "\n",
    "\n",
    "cyto.load_extra_layouts()\n",
    "app = dash.Dash(__name__)\n",
    "server = app.server\n",
    "\n",
    "\n",
    "\n",
    "cy_nodes = build_network_nodes(df_nodes)\n",
    "cy_edges = build_network_edges(df_edges)\n",
    "\n",
    "\n",
    "# define stylesheet\n",
    "n_stylesheet = [\n",
    "    {\n",
    "        \"selector\": 'node',  #For all nodes\n",
    "        'style': {\n",
    "            \"opacity\": 0.9,\n",
    "            \"height\": \"data(weight)\",\n",
    "            'width': 'data(weight)',\n",
    "            \"label\": \"data(label)\",  #Label of node to display\n",
    "            \"background-color\": \"#07ABA0\",  #node color\n",
    "            \"color\": \"#008B80\"  #node label color\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"selector\": 'edge',  #For all edges\n",
    "        \"style\": {\n",
    "            \"target-arrow-color\": \"#C5D3E2\",  #Arrow color\n",
    "            \"target-arrow-shape\": \"triangle\",  #Arrow shape\n",
    "            \"line-color\": \"data(topic_color)\",  #edge color\n",
    "            'arrow-scale': 2,  #Arrow size\n",
    "            'curve-style': 'bezier'  #Default curve-If it is style, the arrow will not be displayed, so specify it\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "page_styles = {\n",
    "    'pre': {\n",
    "        'border': 'thin lightgrey solid',\n",
    "        'overflowX': 'scroll',\n",
    "        'min-height': '50px'\n",
    "    }\n",
    "}\n",
    "\n",
    "edge_legend = topic_color_df[['topic','color']]\n",
    "\n",
    "app.layout = html.Div(children=[\n",
    "    html.H4(children='Communication patters'),\n",
    "\n",
    "    html.Div(\n",
    "        children=[\n",
    "            html.Div(children=[\n",
    "            cyto.Cytoscape(\n",
    "                id='cytoscape',\n",
    "                elements=cy_edges + cy_nodes,\n",
    "                style={\n",
    "                    'height': '95vh',\n",
    "                    'width': '100%'\n",
    "                },\n",
    "                 layout={'name': 'grid'},\n",
    "                stylesheet=n_stylesheet\n",
    "            )], style={'width': '50%'}),\n",
    "            html.Div(children=[\n",
    "                    dcc.Dropdown(id='dropdown_topic',\n",
    "                 options=[{'label': topic.capitalize(), 'value': topic}\n",
    "                         for topic in ['0', '1', '2', '3', '4']\n",
    "                         ]\n",
    "                 ),\n",
    "\n",
    "    dcc.Dropdown(\n",
    "        id='dropdown-update-layout',\n",
    "        options=[\n",
    "            {'label': 'random',\n",
    "             'value': 'random'},\n",
    "            {'label': 'grid',\n",
    "             'value': 'grid'},\n",
    "            {'label': 'circle',\n",
    "             'value': 'circle'},\n",
    "            {'label': 'concentric',\n",
    "             'value': 'concentric'},\n",
    "            {'label': 'breadthfirst - Hiearchy',\n",
    "             'value': 'breadthfirst'},\n",
    "            {'label': 'klay - Force Directed',\n",
    "             'value': 'klay'},\n",
    "            {'label': 'cose - Force Directed',\n",
    "             'value': 'cose'},\n",
    "            {'label': 'cose-bilkent - Force Directed',\n",
    "             'value': 'cose-bilkent'},\n",
    "            {'label': 'cola - Force Directed',\n",
    "             'value': 'cola'},\n",
    "            {'label': 'spread - Force Directed',\n",
    "             'value': 'spread'},\n",
    "            {'label': 'dagre - Hiearchy',\n",
    "             'value': 'dagre'}\n",
    "        ], value='circle'\n",
    "    ),\n",
    "                html.H4(children='NodeData'),\n",
    "                html.P(id='cytoscape_element_info_output'),\n",
    "                html.P(id='cytoscape-tapEdgeData-output'),\n",
    "                html.H4(children='Legend'),\n",
    "                dash_table.DataTable(\n",
    "                    data=edge_legend.to_dict('records'),\n",
    "                    columns=[{\"name\": i, \"id\": i} for i in edge_legend.columns],\n",
    "                    style_cell={'textAlign': 'left'},\n",
    "                    style_data_conditional=[\n",
    "                        {'if': {'row_index': i, 'column_id': 'color'},\n",
    "                         'background-color': edge_legend['color'][i],\n",
    "                         'color': edge_legend['color'][i]} for i in range(edge_legend.shape[0])\n",
    "                    ]\n",
    "                     )\n",
    "        ], style={'width': '50%'})\n",
    "        ], style={'display': 'flex', 'flex-direction': 'row'}),\n",
    "\n",
    "                dash_table.DataTable(\n",
    "                    data=topic_color_df.to_dict('records'),\n",
    "                    columns=[{\"name\": i, \"id\": i} for i in topic_color_df.columns],\n",
    "                    style_cell={'textAlign': 'left'},\n",
    "                    style_data_conditional=[\n",
    "                        {'if': {'row_index': i, 'column_id': 'color'},\n",
    "                         'background-color': topic_color_df['color'][i],\n",
    "                         'color': topic_color_df['color'][i]} for i in range(topic_color_df.shape[0])\n",
    "                    ]\n",
    "                     )\n",
    "])\n",
    "\n",
    "@app.callback(Output('cytoscape', 'layout'),\n",
    "              Input('dropdown-update-layout', 'value'))\n",
    "\n",
    "def update_layout(layout):\n",
    "        return {\n",
    "            'name': layout,\n",
    "        }\n",
    "\n",
    "\n",
    "@app.callback(Output('cytoscape_element_info_output', 'children'),\n",
    "              Input('cytoscape', 'tapNodeData'))\n",
    "\n",
    "def displayTapNodeData(data):\n",
    "    if data:\n",
    "        return \"You recently clicked/tapped node: \" + data['label']\n",
    "\n",
    "\n",
    "@app.callback(Output('cytoscape-tapEdgeData-output', 'children'),\n",
    "              Input('cytoscape', 'tapEdgeData'))\n",
    "\n",
    "def displayTapEdgeData(data):\n",
    "    if data:\n",
    "        return json.dumps(data, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}