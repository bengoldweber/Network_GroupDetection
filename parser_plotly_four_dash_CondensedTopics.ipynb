{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "ERROR: Could not find a version that satisfies the requirement matplotlib_inline.pyplot (from versions: none)\n",
      "ERROR: No matching distribution found for matplotlib_inline.pyplot\n",
      "WARNING: You are using pip version 21.1.2; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bensg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\bensg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with package installation\n",
      "loading outlook messages\n",
      "applying outlook filter\n",
      "building core network arrays\n",
      "error =  <COMObject <unknown>>\n",
      "(687, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bensg\\AppData\\Local\\Temp\\ipykernel_16144\\110458729.py:116: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\genericpath.py:30\u001B[0m, in \u001B[0;36misfile\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 30\u001B[0m     st \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 2] The system cannot find the file specified: 'c:\\\\users\\\\bensg\\\\pycharmprojects\\\\network_groupdetection\\\\venv\\\\lib\\\\nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [29]\u001B[0m, in \u001B[0;36m<cell line: 244>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;66;03m#Grab all messages from outlook folder X that came in during the lasy Y days\u001B[39;00m\n\u001B[0;32m    243\u001B[0m df_messages, df_recipients \u001B[38;5;241m=\u001B[39m outlook_messages_to_pd(folder_to_load, days_back_to_look)\n\u001B[1;32m--> 244\u001B[0m df_messages \u001B[38;5;241m=\u001B[39m \u001B[43mnormalize_df_col_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_messages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;66;03m#save messages to sqlite\u001B[39;00m\n\u001B[0;32m    247\u001B[0m load_core_tables_to_sqlite(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_db_sqlite.db\u001B[39m\u001B[38;5;124m\"\u001B[39m,df_messages, df_recipients)\n",
      "Input \u001B[1;32mIn [29]\u001B[0m, in \u001B[0;36mnormalize_df_col_text\u001B[1;34m(pd_dataframe)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnormalize_df_col_text\u001B[39m(pd_dataframe):\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;66;03m#Text normalization bundled function\u001B[39;00m\n\u001B[0;32m    131\u001B[0m     df_messages \u001B[38;5;241m=\u001B[39m set_lowercase_and_drop_punctuation(pd_dataframe)\n\u001B[1;32m--> 132\u001B[0m     pd_dataframe \u001B[38;5;241m=\u001B[39m \u001B[43mlem_and_stop_removal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_messages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd_dataframe\n",
      "Input \u001B[1;32mIn [29]\u001B[0m, in \u001B[0;36mlem_and_stop_removal\u001B[1;34m(pd_dataframe)\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlem_and_stop_removal\u001B[39m(pd_dataframe):\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;66;03m#Remove stopwords & lemmatize\u001B[39;00m\n\u001B[0;32m    125\u001B[0m     pd_dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtemp_body\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd_dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtemp_body\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m x\u001B[38;5;241m.\u001B[39msplit() \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (stopwords)]))\n\u001B[1;32m--> 126\u001B[0m     pd_dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtemp_body\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mpd_dataframe\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtemp_body\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlemmatize_with_postag\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd_dataframe\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\pandas\\core\\series.py:4433\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4324\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4325\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4328\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4329\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4330\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4331\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4332\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4431\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4432\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4433\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1078\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1079\u001B[0m     \u001B[38;5;66;03m# if we are a string, try to dispatch\u001B[39;00m\n\u001B[0;32m   1080\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m-> 1082\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1131\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001B[39;00m\n\u001B[0;32m   1133\u001B[0m         \u001B[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001B[39;00m\n\u001B[0;32m   1134\u001B[0m         \u001B[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m         \u001B[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001B[39;00m\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;66;03m# \"Callable[[Any], Any]\"\u001B[39;00m\n\u001B[1;32m-> 1137\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1140\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1144\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1145\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Input \u001B[1;32mIn [29]\u001B[0m, in \u001B[0;36mlemmatize_with_postag\u001B[1;34m(sentence)\u001B[0m\n\u001B[0;32m    104\u001B[0m sent \u001B[38;5;241m=\u001B[39m TextBlob(sentence)\n\u001B[0;32m    105\u001B[0m tag_dict \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJ\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    106\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mN\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    107\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mV\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    108\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mR\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m--> 109\u001B[0m words_and_tags \u001B[38;5;241m=\u001B[39m [(w, tag_dict\u001B[38;5;241m.\u001B[39mget(pos[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m'\u001B[39m)) \u001B[38;5;28;01mfor\u001B[39;00m w, pos \u001B[38;5;129;01min\u001B[39;00m \u001B[43msent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtags\u001B[49m]\n\u001B[0;32m    110\u001B[0m lemmatized_list \u001B[38;5;241m=\u001B[39m [wd\u001B[38;5;241m.\u001B[39mlemmatize(tag) \u001B[38;5;28;01mfor\u001B[39;00m wd, tag \u001B[38;5;129;01min\u001B[39;00m words_and_tags]\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(lemmatized_list)\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\textblob\\decorators.py:24\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[1;34m(self, obj, cls)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m---> 24\u001B[0m value \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m value\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\textblob\\blob.py:499\u001B[0m, in \u001B[0;36mBaseBlob.pos_tags\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;124;03m\"\"\"Returns an list of tuples of the form (word, POS tag).\u001B[39;00m\n\u001B[0;32m    489\u001B[0m \n\u001B[0;32m    490\u001B[0m \u001B[38;5;124;03mExample:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    496\u001B[0m \u001B[38;5;124;03m:rtype: list of tuples\u001B[39;00m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, TextBlob):\n\u001B[1;32m--> 499\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [val \u001B[38;5;28;01mfor\u001B[39;00m sublist \u001B[38;5;129;01min\u001B[39;00m [s\u001B[38;5;241m.\u001B[39mpos_tags \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentences] \u001B[38;5;28;01mfor\u001B[39;00m val \u001B[38;5;129;01min\u001B[39;00m sublist]\n\u001B[0;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [(Word(unicode(word), pos_tag\u001B[38;5;241m=\u001B[39mt), unicode(t))\n\u001B[0;32m    502\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m word, t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_tagger\u001B[38;5;241m.\u001B[39mtag(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    503\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m PUNCTUATION_REGEX\u001B[38;5;241m.\u001B[39mmatch(unicode(t))]\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\textblob\\blob.py:499\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;124;03m\"\"\"Returns an list of tuples of the form (word, POS tag).\u001B[39;00m\n\u001B[0;32m    489\u001B[0m \n\u001B[0;32m    490\u001B[0m \u001B[38;5;124;03mExample:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    496\u001B[0m \u001B[38;5;124;03m:rtype: list of tuples\u001B[39;00m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, TextBlob):\n\u001B[1;32m--> 499\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [val \u001B[38;5;28;01mfor\u001B[39;00m sublist \u001B[38;5;129;01min\u001B[39;00m [\u001B[43ms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos_tags\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentences] \u001B[38;5;28;01mfor\u001B[39;00m val \u001B[38;5;129;01min\u001B[39;00m sublist]\n\u001B[0;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [(Word(unicode(word), pos_tag\u001B[38;5;241m=\u001B[39mt), unicode(t))\n\u001B[0;32m    502\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m word, t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_tagger\u001B[38;5;241m.\u001B[39mtag(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    503\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m PUNCTUATION_REGEX\u001B[38;5;241m.\u001B[39mmatch(unicode(t))]\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\textblob\\decorators.py:24\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[1;34m(self, obj, cls)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m---> 24\u001B[0m value \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m value\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\textblob\\blob.py:502\u001B[0m, in \u001B[0;36mBaseBlob.pos_tags\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    499\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [val \u001B[38;5;28;01mfor\u001B[39;00m sublist \u001B[38;5;129;01min\u001B[39;00m [s\u001B[38;5;241m.\u001B[39mpos_tags \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentences] \u001B[38;5;28;01mfor\u001B[39;00m val \u001B[38;5;129;01min\u001B[39;00m sublist]\n\u001B[0;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [(Word(unicode(word), pos_tag\u001B[38;5;241m=\u001B[39mt), unicode(t))\n\u001B[1;32m--> 502\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m word, t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos_tagger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtag\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    503\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m PUNCTUATION_REGEX\u001B[38;5;241m.\u001B[39mmatch(unicode(t))]\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\textblob\\decorators.py:35\u001B[0m, in \u001B[0;36mrequires_nltk_corpus.<locals>.decorated\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorated\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 35\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m     37\u001B[0m         \u001B[38;5;28mprint\u001B[39m(err)\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\textblob\\en\\taggers.py:38\u001B[0m, in \u001B[0;36mNLTKTagger.tag\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text, textblob\u001B[38;5;241m.\u001B[39mcompat\u001B[38;5;241m.\u001B[39mtext_type):\n\u001B[0;32m     36\u001B[0m     text \u001B[38;5;241m=\u001B[39m tb\u001B[38;5;241m.\u001B[39mTextBlob(text)\n\u001B[1;32m---> 38\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtag\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos_tag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001B[0m, in \u001B[0;36mpos_tag\u001B[1;34m(tokens, tagset, lang)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpos_tag\u001B[39m(tokens, tagset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meng\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;124;03m    tag the given list of tokens.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;124;03m    :rtype: list(tuple(str, str))\u001B[39;00m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 165\u001B[0m     tagger \u001B[38;5;241m=\u001B[39m \u001B[43m_get_tagger\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlang\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001B[0m, in \u001B[0;36m_get_tagger\u001B[1;34m(lang)\u001B[0m\n\u001B[0;32m    105\u001B[0m     tagger\u001B[38;5;241m.\u001B[39mload(ap_russian_model_loc)\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 107\u001B[0m     tagger \u001B[38;5;241m=\u001B[39m \u001B[43mPerceptronTagger\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tagger\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001B[0m, in \u001B[0;36mPerceptronTagger.__init__\u001B[1;34m(self, load)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load:\n\u001B[0;32m    166\u001B[0m     AP_MODEL_LOC \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\n\u001B[1;32m--> 167\u001B[0m         \u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mPICKLE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    168\u001B[0m     )\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload(AP_MODEL_LOC)\n",
      "File \u001B[1;32mc:\\users\\bensg\\pycharmprojects\\network_groupdetection\\venv\\lib\\site-packages\\nltk\\data.py:522\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;66;03m# Check each item in our path\u001B[39;00m\n\u001B[0;32m    520\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m path_ \u001B[38;5;129;01min\u001B[39;00m paths:\n\u001B[0;32m    521\u001B[0m     \u001B[38;5;66;03m# Is the path item a zipfile?\u001B[39;00m\n\u001B[1;32m--> 522\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m path_ \u001B[38;5;129;01mand\u001B[39;00m (\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mand\u001B[39;00m path_\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.zip\u001B[39m\u001B[38;5;124m\"\u001B[39m)):\n\u001B[0;32m    523\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    524\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m ZipFilePathPointer(path_, resource_name)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\genericpath.py:30\u001B[0m, in \u001B[0;36misfile\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;124;03m\"\"\"Test whether a path is a regular file\"\"\"\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 30\u001B[0m     st \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import pip\n",
    "\n",
    "def import_or_install(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        pip.main(['install', package])\n",
    "\n",
    "packages = ['PyQt6', 'nltk', 'pyvis' , 'matplotlib_inline' ,'matplotlib', 'gensim.corpora','gensim.utils','gensim.models' , 'matplotlib_inline.pyplot' ,'pandas', 'numpy', 'umap','sqlite3','spacy','win32com.client','datetime','pyvis.network','matplotlib.pyplot','plotly.graph_objects','scipy','networkx','dash','gensim','logging','warnings','nltk.corpus']\n",
    "\n",
    "\n",
    "for package in packages:\n",
    "    import_or_install(package)\n",
    "\n",
    "print('done with package installation')\n",
    "\n",
    "import nltk\n",
    "import sqlite3\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "import win32com.client\n",
    "from datetime import datetime, timedelta\n",
    "from dash import html\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import dash\n",
    "import json\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash import dash_table\n",
    "import dash_cytoscape as cyto\n",
    "\n",
    "def load_outlook_messages(folder_number):\n",
    "    print('loading outlook messages')\n",
    "    outlook = win32com.client.Dispatch(\"Outlook.Application\").GetNamespace(\"MAPI\")\n",
    "    folder_contents = outlook.GetDefaultFolder(folder_number)\n",
    "    outlook_messages = folder_contents.Items\n",
    "    return outlook_messages\n",
    "\n",
    "def outlook_message_filter(message_array, date_range):\n",
    "    print('applying outlook filter')\n",
    "    #filter messages processed by the last X days\n",
    "    received_dt = datetime.now() - timedelta(days=date_range)\n",
    "    received_dt = received_dt.strftime('%m/%d/%Y %H:%M %p')\n",
    "    message_array = message_array.Restrict(\"[ReceivedTime] >= '\" + received_dt + \"'\")\n",
    "    return message_array\n",
    "\n",
    "def build_core_network_arrays(messages):\n",
    "    print('building core network arrays')\n",
    "    #Build core tables\n",
    "    pd_messages = []\n",
    "    pd_recipients = []\n",
    "    for message in list(messages):\n",
    "        try:\n",
    "            recipients_list = message.Recipients\n",
    "            recipients_cleaned = \"\"\n",
    "            for recipient in recipients_list:\n",
    "                pd_recipients.append({\n",
    "                    \"entryID\": str(message.EntryID),\n",
    "                    \"sender\": str(message.Sender),\n",
    "                    \"recipient\": str(recipient),\n",
    "                })\n",
    "\n",
    "            pd_messages.append({\n",
    "                \"entryID\": str(message.EntryID),\n",
    "                \"conversationID\": str(message.ConversationID),\n",
    "                \"conversationIndex\": str(message.ConversationIndex),\n",
    "                \"createTime\": str(message.CreationTime),\n",
    "                \"recievedTime\": str(message.ReceivedTime),\n",
    "                \"ConversationTopic\": str(message.ConversationTopic),\n",
    "                \"subject\": str(message.Subject),\n",
    "                \"body\": str(message.body)\n",
    "            })\n",
    "        except:\n",
    "            print(\"error =  \" + str(recipients_list))\n",
    "\n",
    "    return pd_messages, pd_recipients\n",
    "\n",
    "def core_arrays_to_pd(pd_messages, pd_recipients):\n",
    "    df_messages = pd.DataFrame(pd_messages)\n",
    "    df_recipients = pd.DataFrame(pd_recipients)\n",
    "    return df_messages, df_recipients\n",
    "\n",
    "def outlook_messages_to_pd(folder, days_back):\n",
    "    outlook_messages = load_outlook_messages(folder)\n",
    "    messages = outlook_message_filter(outlook_messages, days_back)\n",
    "    pd_messages, pd_recipients = build_core_network_arrays(messages)\n",
    "    df_messages, df_recipients = core_arrays_to_pd(pd_messages, pd_recipients)\n",
    "    print(df_messages.shape)\n",
    "    return df_messages, df_recipients\n",
    "\n",
    "def lemmatize_with_postag(sentence):\n",
    "    #Get each words Part of speech ( verb, noun etc) then pass it with the word to a lemmatizer. Then put the lemmatized word back into the Pandas DF row\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a',\n",
    "                \"N\": 'n',\n",
    "                \"V\": 'v',\n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]\n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "def set_lowercase_and_drop_punctuation(pd_dataframe):\n",
    "    #dataframe needs to have a body col which it will work on\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['body'].apply(lambda x: x.lower())\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].str.replace(r'[^\\w\\s]+', '')  #(?:\\w+)\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].replace(r'\\n', ' ', regex=True)\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].replace(r'\\r', ' ', regex=True)\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].replace(r'_', '', regex=True)\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].str.strip()\n",
    "    return pd_dataframe\n",
    "\n",
    "def lem_and_stop_removal(pd_dataframe):\n",
    "    #Remove stopwords & lemmatize\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "    pd_dataframe['temp_body'] = pd_dataframe['temp_body'].apply(lemmatize_with_postag)\n",
    "    return pd_dataframe\n",
    "\n",
    "def normalize_df_col_text(pd_dataframe):\n",
    "    #Text normalization bundled function\n",
    "    df_messages = set_lowercase_and_drop_punctuation(pd_dataframe)\n",
    "    pd_dataframe = lem_and_stop_removal(df_messages)\n",
    "    return pd_dataframe\n",
    "\n",
    "def load_core_tables_to_sqlite(db_fileName, message_table, recipient_table):\n",
    "    sql_con = sqlite3.connect(db_fileName)\n",
    "    message_table.to_sql(\"messages\", sql_con, if_exists=\"replace\")\n",
    "    recipient_table.to_sql(\"recipients\", sql_con, if_exists=\"replace\")\n",
    "    sql_con.close()\n",
    "\n",
    "def save_to_sqlite(sqlite_file, table_to_save, tabel_name):\n",
    "    sql_con = sqlite3.connect(sqlite_file)\n",
    "    table_to_save.to_sql(tabel_name, sql_con, if_exists=\"replace\")\n",
    "    sql_con = sqlite3.connect(sqlite_file)\n",
    "    sql_con.close()\n",
    "\n",
    "def load_core_tables_from_sqlite(db_fileName):\n",
    "    #load DF messages & recipient tables into pandas DF's to leverage\n",
    "    sql_con = sqlite3.connect(db_fileName)\n",
    "    message_query = 'select * from messages;'\n",
    "    recipient_query = 'select recipients.* from recipients;'\n",
    "    df_messages = pd.read_sql_query(message_query, sql_con)\n",
    "    df_recipients = pd.read_sql_query(recipient_query, sql_con)\n",
    "    df_recipients = df_recipients.drop('index', 1)\n",
    "    df_messages = df_messages.drop('index', 1)\n",
    "    sql_con.close()\n",
    "    return df_messages, df_recipients\n",
    "\n",
    "def create_topics(pd_messages, topic_count, tfidf, random_state):\n",
    "    dtm = tfidf.fit_transform(df_messages['temp_body'])\n",
    "    nmf_model = NMF(n_components=topic_count, random_state=random_state)\n",
    "    nmf_model.fit(dtm)\n",
    "    print(\"record size : \", len(tfidf.get_feature_names_out()))\n",
    "    return nmf_model, dtm\n",
    "\n",
    "def topics_to_pd(dtm, nmf_model, tfidf, topic_count):\n",
    "    topics_dict={}\n",
    "    for index, topic in enumerate(nmf_model.components_):\n",
    "        topic_text_string =  str([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-topic_count:]])\n",
    "        v = {index: str(topic_text_string)}\n",
    "        topics_dict.update(v)\n",
    "\n",
    "    topic_text_df =  pd.Series(topics_dict).to_frame()\n",
    "    topic_text_df.reset_index(level=0, inplace=True)\n",
    "    topic_text_df = topic_text_df.rename(columns={0: \"topic_text\", 'index': \"topic\"})\n",
    "    topic_results = nmf_model.transform(dtm)\n",
    "    df_messages['topic'] = topic_results.argmax(axis=1)\n",
    "    return df_messages, topic_text_df\n",
    "\n",
    "def generate_topic_colors(topics_pd):\n",
    "    topic_colors = {}\n",
    "    for x in range(topic_count):\n",
    "        hex_color = \"%06x\" % random.randint(0, 0xFFFFFF)\n",
    "        hex_color = \"#\" + hex_color\n",
    "        topic_colors.update({ x: x , x : hex_color,})\n",
    "\n",
    "    topic_color_df = pd.DataFrame(topic_colors.items(), columns=['topic','color'])\n",
    "    topic_color_df = topic_color_df.merge(topic_text_df, left_on=['topic','topic'], right_on=['topic','topic'],  how='left')\n",
    "    return topic_color_df\n",
    "\n",
    "def build_network_nodes(df_nodes):\n",
    "    print('building network node array')\n",
    "    nodes = set()\n",
    "    cy_nodes = []\n",
    "    for index, row in df_nodes.iterrows():\n",
    "        individual, bins = row['individual'], row['log_count']\n",
    "        nodes.add(individual)\n",
    "        cy_nodes.append({\"data\": {\"id\": individual, \"label\": individual, 'weight': bins*10, }})\n",
    "    return cy_nodes\n",
    "\n",
    "def build_network_edges(df_edges):\n",
    "    print('building network edge array')\n",
    "    cy_edges = []\n",
    "    for index, row in df_edges.iterrows():\n",
    "        source, target, topic, weight, topic_color = row['source'], row['target'], row['topic'], row['size'], row['color']\n",
    "        cy_edges.append({\n",
    "            'data': {\n",
    "                'source': source,\n",
    "                'target': target,\n",
    "                'topic': topic,\n",
    "                'weight': weight,\n",
    "                'topic_color': topic_color\n",
    "            }\n",
    "        })\n",
    "    return cy_edges\n",
    "\n",
    "def generate_table(dataframe, max_rows=200):\n",
    "    return html.Table([\n",
    "        html.Thead(\n",
    "            html.Tr([html.Th(col) for col in dataframe.columns])\n",
    "        ),\n",
    "        html.Tbody([\n",
    "            html.Tr([\n",
    "                 html.Td(dataframe.iloc[i][col]) for col in dataframe.columns\n",
    "            ]) for i in range(min(len(dataframe), max_rows))\n",
    "\n",
    "        ])\n",
    "    ])\n",
    "\n",
    "#stopword definitions for later sections\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "#Set Driving Vars\n",
    "topic_count = 15   #number of ML generated topics\n",
    "folder_to_load =  6 #which folders to load from outlook\n",
    "days_back_to_look = 100 #number of days back to pull data for\n",
    "\n",
    "#Grab all messages from outlook folder X that came in during the lasy Y days\n",
    "df_messages, df_recipients = outlook_messages_to_pd(folder_to_load, days_back_to_look)\n",
    "df_messages = normalize_df_col_text(df_messages)\n",
    "\n",
    "#save messages to sqlite\n",
    "load_core_tables_to_sqlite(\"test_db_sqlite.db\",df_messages, df_recipients)\n",
    "\n",
    "#load messages from slite\n",
    "df_messages, df_recipients = load_core_tables_from_sqlite(\"test_db_sqlite.db\")\n",
    "\n",
    "#Generate topics\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', stop_words='english')\n",
    "nmf_model, dtm = create_topics(df_messages, topic_count, tfidf, 42)\n",
    "df_messages, topic_text_df = topics_to_pd(dtm,nmf_model, tfidf, topic_count)\n",
    "topic_color_df = generate_topic_colors(topic_text_df)\n",
    "\n",
    "#THIS CHUNK GENERATES TOPIC COLORS AND MERGES IT WITH THE MASTER TABLEs\n",
    "df_messages = df_messages.merge(topic_color_df, left_on=['topic','topic'], right_on=['topic','topic'],  how='left')\n",
    "df_recipients = df_messages.merge(df_recipients, left_on=['entryID','entryID'], right_on=['entryID','entryID'],  how='left')\n",
    "df_recipients = df_recipients.reset_index(drop=True)\n",
    "\n",
    "df_recipients = df_recipients.groupby(['sender',  'recipient', 'topic', 'color']).size()\n",
    "df_recipients = df_recipients.to_frame(name = 'size').reset_index()\n",
    "df_recipients = df_recipients.loc[df_recipients['size'] != 1]\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "df_recipients.replace(\"\", nan_value, inplace=True)\n",
    "df_recipients = df_recipients.dropna()\n",
    "\n",
    "save_to_sqlite(\"test_db_sqlite.db\",df_recipients,\"compressed_data\")\n",
    "save_to_sqlite(\"test_db_sqlite.db\",topic_color_df,\"topic_color_df\")\n",
    "\n",
    "\n",
    "\n",
    "#Start from here to cut down on processing time\n",
    "sql_con = sqlite3.connect(\"test_db_sqlite.db\")\n",
    "recipient_query = 'select * from compressed_data;'\n",
    "topic_query = 'select * from topic_color_df;'\n",
    "\n",
    "df_recipients = pd.read_sql_query(recipient_query, sql_con)\n",
    "topic_color_df = pd.read_sql_query(topic_query, sql_con)\n",
    "\n",
    "sql_con.close()\n",
    "\n",
    "df_recipients = df_recipients.drop('index', 1)\n",
    "topic_color_df = topic_color_df.drop('index', 1)\n",
    "\n",
    "\n",
    "df_counts = df_recipients.groupby(['sender',  'recipient', 'topic','color']).size().reset_index().rename(columns={0: 'count'})\n",
    "\n",
    "\n",
    "#node df generator\n",
    "df_node_temp =  df_recipients\n",
    "\n",
    "#build node list off of to/from table. Adding a new feature called count driven by node edge counts\n",
    "df1 = df_node_temp['sender']\n",
    "df2 = df_node_temp['recipient']\n",
    "df_nodes = df1.append(df2)\n",
    "df_nodes.reset_index()\n",
    "#df_nodes.set_axis(['name'], axis=1)\n",
    "df_nodes = df_nodes.to_frame()\n",
    "print(\"Dataframe columns:\", df_nodes.columns)\n",
    "\n",
    "df_nodes = df_nodes.rename(columns={0: \"individuals\"})\n",
    "\n",
    "df_nodes = df_nodes['individuals'].value_counts()\n",
    "\n",
    "df_nodes = df_nodes.to_frame()\n",
    "df_nodes = df_nodes.rename(columns={'individuals': \"count\"})\n",
    "df_nodes.index.name = 'individual'\n",
    "df_nodes.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "edges_table = [df_recipients[\"sender\"], df_recipients[\"recipient\"], df_recipients['topic'], df_recipients['color'], df_recipients['size']]\n",
    "headers = [\"source\", \"target\", \"topic\",\"color\", \"size\"]\n",
    "df_edges = pd.concat(edges_table, axis=1, keys=headers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count_mean_nodes = df_nodes['count'].mean()\n",
    "count_std_dev_nodes = df_nodes['count'].std()\n",
    "df_nodes['edge_count_z-score'] = (df_nodes['count'] - count_mean_nodes) / count_std_dev_nodes\n",
    "values_plus_minus_one_z_score = len(df_nodes[df_nodes['edge_count_z-score'].between(-1, 1) == True])\n",
    "percent_values_plus_minus_one_z_score = values_plus_minus_one_z_score / len(df_nodes) * 100\n",
    "z_score_distribution_std_dev = round(df_nodes['edge_count_z-score'].std(), 2)\n",
    "ser = Series(df_nodes['count'])\n",
    "bins = np.quantile(np.unique(ser), np.linspace(0, 1, 11))\n",
    "\n",
    "num_rows = np.shape(bins)[0]\n",
    "bin_count = num_rows\n",
    "starting_count = 5\n",
    "size_labels = range(starting_count, (bin_count + starting_count) - 1, 1)\n",
    "\n",
    "df_nodes['bins'] = pd.cut(ser, bins, include_lowest=True, labels=size_labels)\n",
    "\n",
    "df_nodes['log_count'] = np.log(df_nodes['count'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#THIS CHUNK SAVES TO SQLITE THE DATA PULLED FROM OUTLOOK\n",
    "sql_con = sqlite3.connect(\"test_db_sqlite.db\")\n",
    "df_nodes.to_sql(\"df_nodes\", sql_con, if_exists=\"replace\")\n",
    "df_edges.to_sql(\"df_edges\", sql_con, if_exists=\"replace\")\n",
    "sql_con.close()\n",
    "\n",
    "\n",
    "\n",
    "sql_con = sqlite3.connect(\"test_db_sqlite.db\")\n",
    "edges_q = 'select * from df_edges;'\n",
    "nodes_q = 'select * from df_nodes;'\n",
    "topics_q = 'select * from topic_color_df;'\n",
    "df_edges = pd.read_sql_query(edges_q, sql_con)\n",
    "df_nodes = pd.read_sql_query(nodes_q, sql_con)\n",
    "topic_color_df = pd.read_sql_query(topics_q, sql_con)\n",
    "sql_con.close()\n",
    "\n",
    "\n",
    "\n",
    "df_edges = df_edges.drop('index', 1)\n",
    "df_nodes = df_nodes.drop('index', 1)\n",
    "topic_color_df = topic_color_df.drop('index', 1)\n",
    "\n",
    "\n",
    "cyto.load_extra_layouts()\n",
    "app = dash.Dash(__name__)\n",
    "server = app.server\n",
    "\n",
    "\n",
    "\n",
    "cy_nodes = build_network_nodes(df_nodes)\n",
    "cy_edges = build_network_edges(df_edges)\n",
    "\n",
    "\n",
    "# define stylesheet\n",
    "n_stylesheet = [\n",
    "    {\n",
    "        \"selector\": 'node',  #For all nodes\n",
    "        'style': {\n",
    "            \"opacity\": 0.9,\n",
    "            \"height\": \"data(weight)\",\n",
    "            'width': 'data(weight)',\n",
    "            \"label\": \"data(label)\",  #Label of node to display\n",
    "            \"background-color\": \"#07ABA0\",  #node color\n",
    "            \"color\": \"#008B80\"  #node label color\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"selector\": 'edge',  #For all edges\n",
    "        \"style\": {\n",
    "            \"target-arrow-color\": \"#C5D3E2\",  #Arrow color\n",
    "            \"target-arrow-shape\": \"triangle\",  #Arrow shape\n",
    "            \"line-color\": \"data(topic_color)\",  #edge color\n",
    "            'arrow-scale': 2,  #Arrow size\n",
    "            'curve-style': 'bezier'  #Default curve-If it is style, the arrow will not be displayed, so specify it\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "page_styles = {\n",
    "    'pre': {\n",
    "        'border': 'thin lightgrey solid',\n",
    "        'overflowX': 'scroll',\n",
    "        'min-height': '50px'\n",
    "    }\n",
    "}\n",
    "\n",
    "edge_legend = topic_color_df[['topic','color']]\n",
    "\n",
    "app.layout = html.Div(children=[\n",
    "    html.H4(children='Communication patters'),\n",
    "\n",
    "    html.Div(\n",
    "        children=[\n",
    "            html.Div(children=[\n",
    "            cyto.Cytoscape(\n",
    "                id='cytoscape',\n",
    "                elements=cy_edges + cy_nodes,\n",
    "                style={\n",
    "                    'height': '95vh',\n",
    "                    'width': '100%'\n",
    "                },\n",
    "                 layout={'name': 'grid'},\n",
    "                stylesheet=n_stylesheet\n",
    "            )], style={'width': '50%'}),\n",
    "            html.Div(children=[\n",
    "                    dcc.Dropdown(id='dropdown_topic',\n",
    "                 options=[{'label': topic.capitalize(), 'value': topic}\n",
    "                         for topic in ['0', '1', '2', '3', '4']\n",
    "                         ]\n",
    "                 ),\n",
    "\n",
    "    dcc.Dropdown(\n",
    "        id='dropdown-update-layout',\n",
    "        options=[\n",
    "            {'label': 'random',\n",
    "             'value': 'random'},\n",
    "            {'label': 'grid',\n",
    "             'value': 'grid'},\n",
    "            {'label': 'circle',\n",
    "             'value': 'circle'},\n",
    "            {'label': 'concentric',\n",
    "             'value': 'concentric'},\n",
    "            {'label': 'breadthfirst - Hiearchy',\n",
    "             'value': 'breadthfirst'},\n",
    "            {'label': 'klay - Force Directed',\n",
    "             'value': 'klay'},\n",
    "            {'label': 'cose - Force Directed',\n",
    "             'value': 'cose'},\n",
    "            {'label': 'cose-bilkent - Force Directed',\n",
    "             'value': 'cose-bilkent'},\n",
    "            {'label': 'cola - Force Directed',\n",
    "             'value': 'cola'},\n",
    "            {'label': 'spread - Force Directed',\n",
    "             'value': 'spread'},\n",
    "            {'label': 'dagre - Hiearchy',\n",
    "             'value': 'dagre'}\n",
    "        ], value='circle'\n",
    "    ),\n",
    "                html.H4(children='NodeData'),\n",
    "                html.P(id='cytoscape_element_info_output'),\n",
    "                html.P(id='cytoscape-tapEdgeData-output'),\n",
    "                html.H4(children='Legend'),\n",
    "                dash_table.DataTable(\n",
    "                    data=edge_legend.to_dict('records'),\n",
    "                    columns=[{\"name\": i, \"id\": i} for i in edge_legend.columns],\n",
    "                    style_cell={'textAlign': 'left'},\n",
    "                    style_data_conditional=[\n",
    "                        {'if': {'row_index': i, 'column_id': 'color'},\n",
    "                         'background-color': edge_legend['color'][i],\n",
    "                         'color': edge_legend['color'][i]} for i in range(edge_legend.shape[0])\n",
    "                    ]\n",
    "                     )\n",
    "        ], style={'width': '50%'})\n",
    "        ], style={'display': 'flex', 'flex-direction': 'row'}),\n",
    "\n",
    "                dash_table.DataTable(\n",
    "                    data=topic_color_df.to_dict('records'),\n",
    "                    columns=[{\"name\": i, \"id\": i} for i in topic_color_df.columns],\n",
    "                    style_cell={'textAlign': 'left'},\n",
    "                    style_data_conditional=[\n",
    "                        {'if': {'row_index': i, 'column_id': 'color'},\n",
    "                         'background-color': topic_color_df['color'][i],\n",
    "                         'color': topic_color_df['color'][i]} for i in range(topic_color_df.shape[0])\n",
    "                    ]\n",
    "                     )\n",
    "])\n",
    "\n",
    "@app.callback(Output('cytoscape', 'layout'),\n",
    "              Input('dropdown-update-layout', 'value'))\n",
    "\n",
    "def update_layout(layout):\n",
    "        return {\n",
    "            'name': layout,\n",
    "        }\n",
    "\n",
    "\n",
    "@app.callback(Output('cytoscape_element_info_output', 'children'),\n",
    "              Input('cytoscape', 'tapNodeData'))\n",
    "\n",
    "def displayTapNodeData(data):\n",
    "    if data:\n",
    "        return \"You recently clicked/tapped node: \" + data['label']\n",
    "\n",
    "\n",
    "@app.callback(Output('cytoscape-tapEdgeData-output', 'children'),\n",
    "              Input('cytoscape', 'tapEdgeData'))\n",
    "\n",
    "def displayTapEdgeData(data):\n",
    "    if data:\n",
    "        return json.dumps(data, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}